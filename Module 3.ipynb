{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conolutional Neural Network to execute the OCR:\n",
    "!!! Note it is assumed that for training and testing purposes the images have already been preprocessed with the help of Module 2 !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages Required :\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model:\n",
    "The model's hyperparameters, the input shape and the layers have been defined in the following \n",
    "code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition :\n",
    "\n",
    "# Hyper_parameters:\n",
    "num_classes = 6\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions :\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Model Layers :\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputing the Images:\n",
    "In the following code snippet, provide the path to the directory where the images have been classified ie. the images of a given digit have been put into a single folder with the name of the directory as the digit.\n",
    "The snippet extracts the images from each digit's directory, label's the according to the directory they are in, and then shuffles all the images to provide us with an object of type DirectoryIterator which contains tuples (X, y) : the training images and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the images and label them according to their directory:\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=False,\n",
    "        width_shift_range=0.,  \n",
    "        height_shift_range=0.) \n",
    "\n",
    "# *** Provide the path to the directory with the digit classes here :\n",
    "path_to_read = '/home/ignitarium/Desktop/Text_seg/classes/'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = path_to_read,\n",
    "    target_size = (28, 28),\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - 2s 233ms/step - loss: 1.7793 - acc: 0.2109\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.6624 - acc: 0.3759\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 1.5372 - acc: 0.3722\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 1.3066 - acc: 0.4348\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.3156 - acc: 0.4311\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.2294 - acc: 0.4979\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 1.2702 - acc: 0.4627\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 1.1443 - acc: 0.5434\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 1.1580 - acc: 0.5552\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 1.0679 - acc: 0.6172\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 0.9922 - acc: 0.6002\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.9935 - acc: 0.6906\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 0.8384 - acc: 0.7221\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.8496 - acc: 0.7145\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 0.7493 - acc: 0.7421\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.7041 - acc: 0.7616\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.5995 - acc: 0.7994\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.6632 - acc: 0.7852\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 1s 144ms/step - loss: 0.6004 - acc: 0.8164\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.5916 - acc: 0.7894\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 0.5541 - acc: 0.8112\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.5886 - acc: 0.8268\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.5742 - acc: 0.8189\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 0.4266 - acc: 0.8659\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.4587 - acc: 0.8541\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 0.4574 - acc: 0.8544\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 0.4414 - acc: 0.8722\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 1s 145ms/step - loss: 0.4548 - acc: 0.8672\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 0.4127 - acc: 0.8741\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4203 - acc: 0.8404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd31c06ef10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model:\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing code:\n",
    "Here input the required directory path to 'path_test', where the test images are located.\n",
    "It is assumed that these images have been preprocessed in using Module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing :\n",
    "path_test = '/home/ignitarium/Desktop/Text_seg/Tester/'\n",
    "tester = [cv2.imread(file) for file in glob.glob(path_test+'*.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly more preprocessing :\n",
    "for idx, img in enumerate(tester):\n",
    "    tester[idx] = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "t = np.array(tester)\n",
    "t = np.expand_dims(t, axis = 3)\n",
    "\n",
    "t = t.astype('float32')\n",
    "t /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Reading them:\n",
    "The following line prints the predictions of the images inputed into the tester. Note that the images order in the directory maybe different from that of the order that has been read out of the directory.\n",
    "Hence for verification purposes, one can use plt.imshow(img for img in tester) in the previous snippet.\n",
    "Here, in each line of output, the highest probability index corresponds to the class that has been predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9698830e-03 1.7484095e-05 9.9643672e-01 1.8797712e-06 9.6719712e-04\n",
      "  6.0689624e-04]\n",
      " [9.9983394e-01 6.2196949e-07 1.5029967e-04 7.3206898e-09 8.5121492e-06\n",
      "  6.6664516e-06]\n",
      " [1.0910032e-06 7.0808419e-05 6.0774516e-07 9.9992633e-01 1.1970848e-06\n",
      "  5.1995044e-09]\n",
      " [2.0270981e-02 1.4248400e-02 6.4458437e-02 1.2899865e-03 5.8559664e-02\n",
      "  8.4117252e-01]\n",
      " [2.3050036e-04 9.9886799e-01 2.9538601e-06 4.9540249e-04 3.7400646e-04\n",
      "  2.9186158e-05]\n",
      " [2.7551632e-03 7.7018780e-03 1.5053352e-02 9.4450833e-03 9.6328408e-01\n",
      "  1.7605454e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the model's output:\n",
    "print model.predict(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
